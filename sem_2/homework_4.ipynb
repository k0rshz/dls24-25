{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\" width=\"400\"></p>\n",
        "\n",
        "# Глубокое обучение. Часть 2\n",
        "# Домашнее задание по теме \"Механизм внимания\""
      ],
      "metadata": {
        "id": "Ji8KtYOVGs8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это домашнее задание проходит в формате peer-review. Это означает, что его будут проверять ваши однокурсники. Поэтому пишите разборчивый код, добавляйте комментарии и пишите выводы после проделанной работы.\n",
        "\n",
        "В этом задании вы будете решать задачу классификации математических задач по темам (многоклассовая классификация) с помощью Transformer.\n",
        "\n",
        "В качестве датасета возьмем датасет математических задач по разным темам. Нам необходим следующий файл:\n",
        "\n",
        "[Файл с классами](https://docs.google.com/spreadsheets/d/13YIbphbWc62sfa-bCh8MLQWKizaXbQK9/edit?usp=drive_link&ouid=104379615679964018037&rtpof=true&sd=true)"
      ],
      "metadata": {
        "id": "UAr-M8_1GJ6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hint:** не перезаписывайте модели, которые вы получите на каждом из этапов этого дз. Они ещё понадобятся."
      ],
      "metadata": {
        "id": "1fybMcmV0YRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"/content/data_problems.xlsx\", index_col=0)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "HkdGNNlVbGFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.2)"
      ],
      "metadata": {
        "id": "7F23fy8tbXeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "gXvoSwiabWbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Кастомный Dataset\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, label_to_id, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_to_id = label_to_id\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label_str = self.labels[idx]\n",
        "        label = self.label_to_id[label_str]  # Преобразуем строковую метку в число\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def prepare_data(df, train_df, val_df, model_name, feat_name, target, max_length=512):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    unique_labels = sorted(df[target].unique())\n",
        "    label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
        "    id_to_label = {i: label for label, i in label_to_id.items()}\n",
        "\n",
        "    train_dataset = TextClassificationDataset(\n",
        "        train_df[feat_name].tolist(),\n",
        "        train_df[target].tolist(),\n",
        "        tokenizer,\n",
        "        label_to_id,\n",
        "        max_length\n",
        "    )\n",
        "\n",
        "    val_dataset = TextClassificationDataset(\n",
        "        val_df[feat_name].tolist(),\n",
        "        val_df[target].tolist(),\n",
        "        tokenizer,\n",
        "        label_to_id,\n",
        "        max_length\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset, label_to_id, id_to_label, tokenizer"
      ],
      "metadata": {
        "id": "4DOc5x3Ad9PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 1 (2 балла)\n",
        "\n",
        "Напишите кастомный класс для модели трансформера для задачи классификации, использующей в качествке backbone какую-то из моделей huggingface.\n",
        "\n",
        "Т.е. конструктор класса должен принимать на вход название модели и подгружать её из huggingface, а затем использовать в качестве backbone (достаточно возможности использовать в качестве backbone те модели, которые упомянуты в последующих пунктах)"
      ],
      "metadata": {
        "id": "t395freCxpOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### This is just an interface example. You may change it if you want.\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoConfig\n",
        "\n",
        "class TransformerClassificationModel(nn.Module):\n",
        "    def __init__(self, base_transformer_model, dropout, num_classes):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(base_transformer_model) #...\n",
        "        # YOUR CODE: create additional layers for classfication\n",
        "        hidden_size = self.backbone.config.hidden_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None, **kwargs):\n",
        "        # YOUR CODE: propagate inputs through the model. Return dict with logits\n",
        "        transformer_out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        outputs = transformer_out.last_hidden_state[:, 0]  # [CLS] токен\n",
        "        logits = self.classifier(self.dropout(outputs))\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(logits, labels)\n",
        "        # Возвращаем словарь как ожидает Trainer\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"logits\": logits\n",
        "        }"
      ],
      "metadata": {
        "id": "eX4VGWquyiMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 2 (1 балл)\n",
        "\n",
        "Напишите функцию заморозки backbone у модели (если необходимо, возвращайте из функции модель)"
      ],
      "metadata": {
        "id": "Vd3kxX6hy0d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_backbone_function(model: TransformerClassificationModel):\n",
        "    for param in model.backbone.parameters():\n",
        "        param.requires_grad = False\n",
        "    return model"
      ],
      "metadata": {
        "id": "U8IuDosbzKe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 3 (2 балла)\n",
        "\n",
        "Напишите функцию, которая будет использована для тренировки (дообучения) трансформера (TransformerClassificationModel). Функция должна поддерживать обучение с замороженным и размороженным backbone."
      ],
      "metadata": {
        "id": "kybkw6MSzd-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def train_transformer(transformer_model, train_dataset, val_dataset, freeze_backbone):\n",
        "    model = copy.deepcopy(transformer_model)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=2,\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=150,\n",
        "        logging_steps=150\n",
        "    )\n",
        "\n",
        "    if freeze_backbone:\n",
        "        freeze_backbone_function(model)\n",
        "\n",
        "    # Создаем Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    # Запускаем обучение\n",
        "    trainer.train()\n",
        "\n",
        "    # Возвращаем обученную модель\n",
        "    return trainer.model"
      ],
      "metadata": {
        "id": "EDhrD0BHzxi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 4 (1 балл)\n",
        "\n",
        "Проверьте вашу функцию из предыдущего пункта, дообучив двумя способами\n",
        "*cointegrated/rubert-tiny2* из huggingface."
      ],
      "metadata": {
        "id": "eUqhI4mV_RTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"cointegrated/rubert-tiny2\"\n",
        "train_dataset, val_dataset, label_to_id, id_to_label, tokenizer = prepare_data(\n",
        "    df, train_df, val_df, model_name, \"Задача\", \"Тема\"\n",
        ")\n",
        "num_classes = len(label_to_id)"
      ],
      "metadata": {
        "id": "D0uDBsCFfFul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\""
      ],
      "metadata": {
        "id": "sYyVlkcWgVw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rubert_tiny_transformer_model_1 = TransformerClassificationModel(model_name, dropout=0.2, num_classes=num_classes)\n",
        "rubert_tiny_finetuned_with_freezed_backbone = train_transformer(rubert_tiny_transformer_model_1, train_dataset, val_dataset,\n",
        "                                                                freeze_backbone=True)\n",
        "\n",
        "rubert_tiny_transformer_model_2 = TransformerClassificationModel(model_name, dropout=0.2, num_classes=num_classes)\n",
        "rubert_tiny_full_finetuned = train_transformer(rubert_tiny_transformer_model_2, train_dataset, val_dataset,\n",
        "                                                                freeze_backbone=False)"
      ],
      "metadata": {
        "id": "nuxOCBQHAKZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 5 (1 балл)\n",
        "\n",
        "Обучите *tbs17/MathBert* (с замороженным backbone и без заморозки), проанализируйте результаты. Сравните скоры с первым заданием. Получилось лучше или нет? Почему?"
      ],
      "metadata": {
        "id": "zRi7tkoOAjon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_eng = pd.read_csv(\"/content/data_problems_translated.xlsx - Sheet1.csv\", index_col=0)\n",
        "train_df_eng, val_df_eng = train_test_split(df_eng, test_size=0.2)\n",
        "df_eng.head()"
      ],
      "metadata": {
        "id": "sNo0VRRYyT0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_eng = \"tbs17/MathBert\"\n",
        "train_dataset_eng, val_dataset_eng, label_to_id_eng, id_to_label_eng, tokenizer_eng = prepare_data(\n",
        "    df_eng, train_df_eng, val_df_eng, model_name_eng, \"problem_text\", \"topic\"\n",
        ")\n",
        "num_classes = len(label_to_id)"
      ],
      "metadata": {
        "id": "3MmLJe95qZ5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE IS HERE (probably, similar on the previous step)\n",
        "MathBert_transformer_model_1 = TransformerClassificationModel(model_name_eng, dropout=0.2, num_classes=num_classes)\n",
        "MathBert_finetuned_with_freezed_backbone = train_transformer(MathBert_transformer_model_1, train_dataset_eng, val_dataset_eng,\n",
        "                                                                freeze_backbone=True)\n",
        "\n",
        "MathBert_transformer_model_2 = TransformerClassificationModel(model_name_eng, dropout=0.2, num_classes=num_classes)\n",
        "MathBert_full_finetuned = train_transformer(MathBert_transformer_model_2, train_dataset_eng, val_dataset_eng,\n",
        "                                                                freeze_backbone=False)"
      ],
      "metadata": {
        "id": "XKtd3YgNA14E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тут уже в конце переобучилась моделька..."
      ],
      "metadata": {
        "id": "xPZlndtWENay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 6 (1 балл)\n",
        "\n",
        "Напишите функцию для отрисовки карт внимания первого слоя для моделей из задания"
      ],
      "metadata": {
        "id": "EuU6Di26017B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertviz -q"
      ],
      "metadata": {
        "id": "vKYDSeZZ2ugT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertviz import head_view\n",
        "from bertviz import model_view\n",
        "\n",
        "def bertviz_plot(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = model.backbone(**inputs, output_attentions=True)\n",
        "\n",
        "    # Получение матрицы внимания\n",
        "    attention = outputs.attentions[0].unsqueeze(0)\n",
        "\n",
        "    # Визуализация внимания с помощью head_view\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    head_view(attention, tokens)\n",
        "\n",
        "    # Визуализация всего слоя модели\n",
        "    model_view(attention, tokens)\n"
      ],
      "metadata": {
        "id": "tBTG7bgY2p9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_attention_for_random_samples(val_df, val_df_eng, model_ru, model_math, tokenizer_ru, tokenizer_math, num_samples=3):\n",
        "    \"\"\"\n",
        "    Выбирает случайные примеры из val_df и val_df_eng и визуализирует attention heads\n",
        "\n",
        "    Args:\n",
        "        val_df: Russian validation DataFrame\n",
        "        val_df_eng: English validation DataFrame\n",
        "        model_ru: Russian model (RuBERT-Tiny2)\n",
        "        model_math: MathBERT model\n",
        "        tokenizer_ru: Russian tokenizer\n",
        "        tokenizer_math: MathBERT tokenizer\n",
        "        num_samples: Number of samples to analyze\n",
        "    \"\"\"\n",
        "\n",
        "    # Выбираем случайные примеры\n",
        "    ru_samples = val_df.sample(n=num_samples, random_state=42)\n",
        "    eng_samples = val_df_eng.sample(n=num_samples, random_state=42)\n",
        "\n",
        "    print(\"=== РУССКИЕ ТЕКСТЫ ===\")\n",
        "    for i in range(num_samples):\n",
        "        bertviz_plot(ru_samples.iloc[i]['Задача'], tokenizer_ru, model_ru)\n",
        "\n",
        "    print(\"\\n=== АНГЛИЙСКИЕ ТЕКСТЫ ===\")\n",
        "    for i in range(num_samples):\n",
        "        bertviz_plot(eng_samples.iloc[i]['problem_text'], tokenizer_eng, model_math)"
      ],
      "metadata": {
        "id": "guzGxfcV1Cba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Задание 7 (1 балл)\n",
        "\n",
        "Проведите инференс для всех моделей **ДО ДООБУЧЕНИЯ** на 2-3 текстах из датасета. Посмотрите на головы Attention первого слоя в каждой модели на выбранных текстах (отрисуйте их отдельно).\n",
        "\n",
        "Попробуйте их проинтерпретировать. Какие связи улавливают карты внимания? (если в модели много голов Attention, то проинтерпретируйте наиболее интересные)"
      ],
      "metadata": {
        "id": "Iu0adKw4BLtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Запуск анализа\n",
        "plot_attention_for_random_samples(\n",
        "    val_df=val_df,\n",
        "    val_df_eng=val_df_eng,\n",
        "    model_ru=rubert_tiny_finetuned_with_freezed_backbone,\n",
        "    model_math=MathBert_finetuned_with_freezed_backbone,\n",
        "    tokenizer_ru=tokenizer,\n",
        "    tokenizer_math=tokenizer_eng,\n",
        "    num_samples=1  # по 1 примеру из каждого датафрейма\n",
        ")"
      ],
      "metadata": {
        "id": "U2gEF3vkB6eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FPvoi66RExlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 8 (1 балл)\n",
        "\n",
        "Сделайте то же самое для дообученных моделей. Изменились ли карты внимания и связи, которые они улавливают? Почему?"
      ],
      "metadata": {
        "id": "pBNVrOpCCLqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE IS HERE\n",
        "\n",
        "# Возьму полностью рамороженные дообученные модели\n",
        "plot_attention_for_random_samples(\n",
        "    val_df=val_df,\n",
        "    val_df_eng=val_df_eng,\n",
        "    model_ru=rubert_tiny_full_finetuned,\n",
        "    model_math=MathBert_full_finetuned,\n",
        "    tokenizer_ru=tokenizer,\n",
        "    tokenizer_math=tokenizer_eng,\n",
        "    num_samples=1  # по 1 примеру из каждого датафрейма\n",
        ")"
      ],
      "metadata": {
        "id": "F5229WBICWEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oOdOSi-xBkIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}